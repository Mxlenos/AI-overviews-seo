"""
Batch Ä°ÅŸleme ModÃ¼lÃ¼ - AI Overview Projesi
Ham web sitesi verilerini 50'ÅŸer URL'lik batch'lere bÃ¶ler ve iÅŸler.
"""

import os
import sys
import json
import time
from pathlib import Path
from typing import List, Dict
import logging
from math import ceil

# Proje kÃ¶k dizinini sys.path'e ekle
sys.path.append(str(Path(__file__).parent.parent))
from config.settings import *

# Loglama konfigÃ¼rasyonu
logging.basicConfig(level=LOG_LEVEL, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class BatchProcessor:
    """Veri batch iÅŸleme sÄ±nÄ±fÄ±"""
    
    def __init__(self):
        self.batches = []
        self.metadata = {}
        
    def load_raw_data(self, raw_data_file: Path) -> List[Dict]:
        """Ham veri dosyasÄ±nÄ± yÃ¼kler"""
        logger.info(f"Ham veri yÃ¼kleniyor: {raw_data_file}")
        
        with open(raw_data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        logger.info(f"{len(data)} kayÄ±t yÃ¼klendi")
        return data
    
    def clean_and_validate_data(self, data: List[Dict]) -> List[Dict]:
        """Veriyi temizler ve doÄŸrular"""
        cleaned_data = []
        
        for item in data:
            # Temel alanlarÄ± kontrol et
            if not item.get('url') or not item.get('content'):
                logger.warning(f"Eksik veri atlanÄ±yor: {item.get('url', 'URL yok')}")
                continue
            
            # Ä°Ã§erik uzunluÄŸu kontrolÃ¼
            word_count = len(item['content'].split())
            if word_count < CONTENT_OPTIMIZATION_RULES['min_word_count']:
                logger.warning(f"Ã‡ok kÄ±sa iÃ§erik atlanÄ±yor: {item['url']} ({word_count} kelime)")
                continue
            
            # Ã‡ok uzun iÃ§erikleri kÄ±salt
            if word_count > CONTENT_OPTIMIZATION_RULES['max_word_count']:
                words = item['content'].split()[:CONTENT_OPTIMIZATION_RULES['max_word_count']]
                item['content'] = ' '.join(words)
                item['word_count'] = len(words)
                logger.info(f"Ä°Ã§erik kÄ±saltÄ±ldÄ±: {item['url']} ({word_count} -> {len(words)} kelime)")
            
            # BaÅŸlÄ±k kontrolÃ¼
            if not item.get('title'):
                item['title'] = f"Sayfa - {item['url'].split('/')[-1]}"
            
            # Meta aÃ§Ä±klama kontrolÃ¼
            if not item.get('meta_description'):
                # Ä°Ã§eriÄŸin ilk 160 karakterini kullan
                item['meta_description'] = item['content'][:160] + "..." if len(item['content']) > 160 else item['content']
            
            cleaned_data.append(item)
        
        logger.info(f"Veri temizleme tamamlandÄ±: {len(data)} -> {len(cleaned_data)} kayÄ±t")
        return cleaned_data
    
    def create_batches(self, data: List[Dict]) -> List[List[Dict]]:
        """Veriyi batch'lere bÃ¶ler"""
        batches = []
        total_batches = ceil(len(data) / MAX_URLS_PER_BATCH)
        
        logger.info(f"Veri {total_batches} batch'e bÃ¶lÃ¼nÃ¼yor (her batch'te {MAX_URLS_PER_BATCH} URL)")
        
        for i in range(0, len(data), MAX_URLS_PER_BATCH):
            batch = data[i:i + MAX_URLS_PER_BATCH]
            batches.append(batch)
        
        return batches
    
    def optimize_content_for_ai(self, content: str, title: str = "") -> str:
        """Ä°Ã§eriÄŸi AI Overview iÃ§in optimize eder"""
        # BaÅŸlÄ±k varsa iÃ§eriÄŸe ekle
        if title:
            optimized_content = f"# {title}\n\n{content}"
        else:
            optimized_content = content
        
        # ParagraflarÄ± ayÄ±r ve temizle
        paragraphs = [p.strip() for p in optimized_content.split('\n') if p.strip()]
        
        # Ã‡ok kÄ±sa paragraflarÄ± birleÅŸtir
        merged_paragraphs = []
        current_paragraph = ""
        
        for paragraph in paragraphs:
            if len(paragraph.split()) < 20 and current_paragraph:
                current_paragraph += " " + paragraph
            else:
                if current_paragraph:
                    merged_paragraphs.append(current_paragraph)
                current_paragraph = paragraph
        
        if current_paragraph:
            merged_paragraphs.append(current_paragraph)
        
        return '\n\n'.join(merged_paragraphs)
    
    def create_batch_metadata(self, batch_id: str, batch_data: List[Dict]) -> Dict:
        """Batch metadata'sÄ±nÄ± oluÅŸturur"""
        urls = [item['url'] for item in batch_data]
        total_words = sum(item['word_count'] for item in batch_data)
        
        metadata = {
            'batch_id': batch_id,
            'created_at': time.strftime('%Y-%m-%d %H:%M:%S'),
            'url_count': len(batch_data),
            'urls': urls,
            'total_words': total_words,
            'average_words_per_page': total_words // len(batch_data) if batch_data else 0,
            'file_format': BATCH_FILE_FORMAT,
            'processing_status': 'created'
        }
        
        return metadata
    
    def save_batch_as_jsonl(self, batch_data: List[Dict], batch_id: str) -> Path:
        """Batch'i JSONL formatÄ±nda kaydeder"""
        filename = f"batch_{batch_id}.{BATCH_FILE_FORMAT}"
        filepath = BATCHES_DIR / filename
        filepath.parent.mkdir(parents=True, exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            for item in batch_data:
                # AI Overview iÃ§in optimize et
                optimized_item = item.copy()
                optimized_item['content'] = self.optimize_content_for_ai(
                    item['content'], 
                    item.get('title', '')
                )
                
                # JSONL formatÄ±nda yaz (her satÄ±rda bir JSON)
                json.dump(optimized_item, f, ensure_ascii=False)
                f.write('\n')
        
        logger.info(f"Batch kaydedildi: {filepath}")
        return filepath
    
    def save_metadata(self, all_metadata: List[Dict]) -> Path:
        """TÃ¼m batch metadata'larÄ±nÄ± kaydeder"""
        metadata_file = BATCHES_DIR / "batches_metadata.json"
        
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(all_metadata, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Metadata kaydedildi: {metadata_file}")
        return metadata_file
    
    def process_data_to_batches(self, raw_data_file: Path) -> Dict:
        """Ana iÅŸleme fonksiyonu"""
        logger.info("Batch iÅŸleme baÅŸlÄ±yor...")
        
        # Ham veriyi yÃ¼kle
        raw_data = self.load_raw_data(raw_data_file)
        
        # Veriyi temizle ve doÄŸrula
        cleaned_data = self.clean_and_validate_data(raw_data)
        
        if not cleaned_data:
            raise ValueError("Ä°ÅŸlenecek geÃ§erli veri bulunamadÄ±")
        
        # Batch'lere bÃ¶l
        batches = self.create_batches(cleaned_data)
        
        # Her batch'i iÅŸle ve kaydet
        batch_files = []
        all_metadata = []
        
        for i, batch_data in enumerate(batches, 1):
            batch_id = f"{int(time.time())}_{i:03d}"
            
            # Batch dosyasÄ±nÄ± kaydet
            batch_file = self.save_batch_as_jsonl(batch_data, batch_id)
            batch_files.append(batch_file)
            
            # Metadata oluÅŸtur
            metadata = self.create_batch_metadata(batch_id, batch_data)
            all_metadata.append(metadata)
            
            logger.info(f"Batch {i}/{len(batches)} iÅŸlendi: {len(batch_data)} URL")
        
        # Metadata'yÄ± kaydet
        metadata_file = self.save_metadata(all_metadata)
        
        # Ã–zet rapor
        summary = {
            'total_pages_processed': len(cleaned_data),
            'total_batches_created': len(batches),
            'batch_files': [str(f) for f in batch_files],
            'metadata_file': str(metadata_file),
            'processing_date': time.strftime('%Y-%m-%d %H:%M:%S'),
            'average_pages_per_batch': len(cleaned_data) / len(batches) if batches else 0
        }
        
        logger.info("Batch iÅŸleme tamamlandÄ±!")
        return summary

def main():
    """Ana fonksiyon - komut satÄ±rÄ±ndan Ã§alÄ±ÅŸtÄ±rma"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Veri batch iÅŸleme')
    parser.add_argument('--input', required=True, help='Ham veri dosyasÄ± yolu')
    parser.add_argument('--output-dir', help='Ã‡Ä±ktÄ± dizini (varsayÄ±lan: data/batches)')
    
    args = parser.parse_args()
    
    # Dosya kontrolÃ¼
    input_file = Path(args.input)
    if not input_file.exists():
        print(f"âŒ Hata: Dosya bulunamadÄ±: {input_file}")
        return
    
    # Ã‡Ä±ktÄ± dizini ayarla
    if args.output_dir:
        global BATCHES_DIR
        BATCHES_DIR = Path(args.output_dir)
    
    # Batch iÅŸleme
    processor = BatchProcessor()
    try:
        summary = processor.process_data_to_batches(input_file)
        
        print(f"\nâœ… Batch iÅŸleme tamamlandÄ±!")
        print(f"ğŸ“Š Ä°ÅŸlenen sayfa sayÄ±sÄ±: {summary['total_pages_processed']}")
        print(f"ğŸ“¦ OluÅŸturulan batch sayÄ±sÄ±: {summary['total_batches_created']}")
        print(f"ğŸ“„ Ortalama batch bÃ¼yÃ¼klÃ¼ÄŸÃ¼: {summary['average_pages_per_batch']:.1f} sayfa")
        print(f"ğŸ“ Batch dosyalarÄ±: {BATCHES_DIR}")
        print(f"ğŸ“‹ Metadata dosyasÄ±: {summary['metadata_file']}")
        
    except Exception as e:
        logger.error(f"Batch iÅŸleme hatasÄ±: {str(e)}")
        print(f"âŒ Hata: {str(e)}")

if __name__ == "__main__":
    main() 