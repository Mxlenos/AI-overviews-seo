"""
Veri Ã‡Ä±karma ModÃ¼lÃ¼ - AI Overview Projesi
Web sitelerinden iÃ§erik Ã§Ä±karÄ±r ve yapÄ±landÄ±rÄ±r.
"""

import os
import sys
import requests
import json
import time
from pathlib import Path
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
import logging
from tqdm import tqdm

# Proje kÃ¶k dizinini sys.path'e ekle
sys.path.append(str(Path(__file__).parent.parent))
from config.settings import *

# Loglama konfigÃ¼rasyonu
logging.basicConfig(level=LOG_LEVEL, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class WebsiteDataExtractor:
    """Web sitesi veri Ã§Ä±karma sÄ±nÄ±fÄ±"""
    
    def __init__(self, base_url: str, max_pages: int = 100):
        self.base_url = base_url
        self.max_pages = max_pages
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.extracted_data = []
        
    def get_page_content(self, url: str) -> Optional[Dict]:
        """Tek bir sayfadan iÃ§erik Ã§Ä±karÄ±r"""
        try:
            time.sleep(REQUEST_DELAY)  # Rate limiting
            response = self.session.get(url, timeout=TIMEOUT)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Meta bilgileri Ã§Ä±kar
            title = soup.find('title')
            title_text = title.get_text().strip() if title else ""
            
            meta_description = soup.find('meta', attrs={'name': 'description'})
            description = meta_description.get('content', '') if meta_description else ""
            
            # Ana iÃ§eriÄŸi Ã§Ä±kar - yaygÄ±n HTML etiketlerini kullan
            content_selectors = [
                'article', 'main', '.content', '#content', 
                '.post-content', '.entry-content', '.article-content'
            ]
            
            content = ""
            for selector in content_selectors:
                content_element = soup.select_one(selector)
                if content_element:
                    content = content_element.get_text(separator=' ', strip=True)
                    break
            
            # EÄŸer ana iÃ§erik bulunamazsa, body'den Ã§Ä±kar
            if not content:
                body = soup.find('body')
                if body:
                    # Navigasyon, footer gibi Ã¶ÄŸeleri kaldÄ±r
                    for tag in body(['nav', 'footer', 'header', 'aside', 'script', 'style']):
                        tag.decompose()
                    content = body.get_text(separator=' ', strip=True)
            
            # BaÅŸlÄ±klarÄ± Ã§Ä±kar
            headings = []
            for i in range(1, 7):  # h1-h6
                for heading in soup.find_all(f'h{i}'):
                    headings.append({
                        'level': i,
                        'text': heading.get_text().strip()
                    })
            
            # BaÄŸlantÄ±larÄ± Ã§Ä±kar
            links = []
            for link in soup.find_all('a', href=True):
                href = link['href']
                if href.startswith('http') or href.startswith('/'):
                    absolute_url = urljoin(url, href)
                    links.append({
                        'url': absolute_url,
                        'text': link.get_text().strip()
                    })
            
            return {
                'url': url,
                'title': title_text,
                'meta_description': description,
                'content': content,
                'headings': headings,
                'links': links,
                'word_count': len(content.split()),
                'extracted_at': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            
        except Exception as e:
            logger.error(f"Sayfa Ã§Ä±karma hatasÄ± {url}: {str(e)}")
            return None
    
    def discover_pages(self, start_url: str) -> List[str]:
        """Web sitesindeki sayfalarÄ± keÅŸfeder"""
        discovered_urls = set()
        to_crawl = [start_url]
        crawled = set()
        
        domain = urlparse(start_url).netloc
        
        while to_crawl and len(discovered_urls) < self.max_pages:
            current_url = to_crawl.pop(0)
            
            if current_url in crawled:
                continue
                
            crawled.add(current_url)
            logger.info(f"Sayfa keÅŸfediliyor: {current_url}")
            
            try:
                response = self.session.get(current_url, timeout=TIMEOUT)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                discovered_urls.add(current_url)
                
                # AynÄ± domain'deki baÄŸlantÄ±larÄ± bul
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    absolute_url = urljoin(current_url, href)
                    parsed_url = urlparse(absolute_url)
                    
                    # AynÄ± domain ve desteklenen formatlar
                    if (parsed_url.netloc == domain and 
                        absolute_url not in crawled and 
                        absolute_url not in to_crawl and
                        len(discovered_urls) < self.max_pages):
                        
                        # PDF, dosya indirme linklerini filtrele
                        if not any(absolute_url.lower().endswith(ext) 
                                 for ext in ['.pdf', '.doc', '.zip', '.jpg', '.png']):
                            to_crawl.append(absolute_url)
                
                time.sleep(REQUEST_DELAY)
                
            except Exception as e:
                logger.error(f"Sayfa keÅŸif hatasÄ± {current_url}: {str(e)}")
                continue
        
        return list(discovered_urls)
    
    def extract_website_data(self, start_url: str) -> List[Dict]:
        """TÃ¼m web sitesi verilerini Ã§Ä±karÄ±r"""
        logger.info(f"Web sitesi veri Ã§Ä±karma baÅŸlÄ±yor: {start_url}")
        
        # SayfalarÄ± keÅŸfet
        urls = self.discover_pages(start_url)
        logger.info(f"{len(urls)} sayfa keÅŸfedildi")
        
        # Her sayfadan veri Ã§Ä±kar
        extracted_data = []
        for url in tqdm(urls, desc="Sayfalar iÅŸleniyor"):
            page_data = self.get_page_content(url)
            if page_data:
                extracted_data.append(page_data)
        
        logger.info(f"{len(extracted_data)} sayfa baÅŸarÄ±yla iÅŸlendi")
        return extracted_data
    
    def save_raw_data(self, data: List[Dict], filename: str = None):
        """Ham veriyi dosyaya kaydet"""
        if not filename:
            timestamp = time.strftime('%Y%m%d_%H%M%S')
            filename = f"raw_website_data_{timestamp}.json"
        
        filepath = RAW_DATA_DIR / filename
        filepath.parent.mkdir(parents=True, exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Ham veri kaydedildi: {filepath}")
        return filepath

def main():
    """Ana fonksiyon - komut satÄ±rÄ±ndan Ã§alÄ±ÅŸtÄ±rma"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Web sitesi veri Ã§Ä±karma')
    parser.add_argument('--url', required=True, help='Hedef web sitesi URL')
    parser.add_argument('--max-pages', type=int, default=100, help='Maksimum sayfa sayÄ±sÄ±')
    parser.add_argument('--output', help='Ã‡Ä±ktÄ± dosya adÄ±')
    
    args = parser.parse_args()
    
    # Veri Ã§Ä±karma
    extractor = WebsiteDataExtractor(args.url, args.max_pages)
    data = extractor.extract_website_data(args.url)
    
    # Kaydetme
    output_file = extractor.save_raw_data(data, args.output)
    
    print(f"\nâœ… Veri Ã§Ä±karma tamamlandÄ±!")
    print(f"ğŸ“Š Ä°ÅŸlenen sayfa sayÄ±sÄ±: {len(data)}")
    print(f"ğŸ’¾ Ã‡Ä±ktÄ± dosyasÄ±: {output_file}")
    
    # Ã–zet istatistikler
    total_words = sum(item['word_count'] for item in data)
    print(f"ğŸ“ Toplam kelime sayÄ±sÄ±: {total_words:,}")
    print(f"ğŸ“„ Ortalama sayfa uzunluÄŸu: {total_words // len(data) if data else 0} kelime")

if __name__ == "__main__":
    main() 