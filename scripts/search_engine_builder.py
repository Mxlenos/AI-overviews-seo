"""
Arama Motoru Kurucusu - AI Overview Projesi
Vertex AI Search Ã¼zerinde Ã¶zel arama motoru oluÅŸturur ve AI Overview analizleri yapar.
"""

import os
import sys
import json
import time
from pathlib import Path
from typing import List, Dict, Optional, Any
import logging
from datetime import datetime, timedelta

# Google Cloud kÃ¼tÃ¼phaneleri
try:
    from google.cloud import discoveryengine
    from google.cloud import storage
    from google.auth import default
except ImportError as e:
    print("âŒ Google Cloud kÃ¼tÃ¼phaneleri bulunamadÄ±. LÃ¼tfen requirements.txt'i yÃ¼kleyin:")
    print("pip install -r requirements.txt")
    sys.exit(1)

# AI analiz iÃ§in kÃ¼tÃ¼phaneler
try:
    import pandas as pd
    import numpy as np
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    from sentence_transformers import SentenceTransformer
except ImportError as e:
    print("âŒ AI analiz kÃ¼tÃ¼phaneleri bulunamadÄ±. LÃ¼tfen requirements.txt'i yÃ¼kleyin:")
    print("pip install -r requirements.txt")
    sys.exit(1)

# Proje kÃ¶k dizinini sys.path'e ekle
sys.path.append(str(Path(__file__).parent.parent))
from config.settings import *

# Loglama konfigÃ¼rasyonu
logging.basicConfig(level=LOG_LEVEL, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SearchEngineBuilder:
    """Arama motoru kurucusu ve AI Overview analiz sÄ±nÄ±fÄ±"""
    
    def __init__(self):
        self.project_id = GCP_PROJECT_ID
        self.location = DISCOVERY_ENGINE_LOCATION
        self.storage_client = None
        self.search_client = None
        self.document_client = None
        self.sentence_model = None
        self._initialize_clients()
        
    def _initialize_clients(self):
        """Google Cloud client'larÄ±nÄ± baÅŸlat"""
        try:
            # Kimlik doÄŸrulama
            credentials, project = default()
            logger.info(f"Google Cloud kimlik doÄŸrulamasÄ± baÅŸarÄ±lÄ±: {project}")
            
            # Client'larÄ± baÅŸlat
            self.storage_client = storage.Client(project=self.project_id)
            self.search_client = discoveryengine.SearchServiceClient()
            self.document_client = discoveryengine.DocumentServiceClient()
            
            # Sentence transformer modelini yÃ¼kle
            self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
            
            logger.info("TÃ¼m client'lar baÅŸarÄ±yla baÅŸlatÄ±ldÄ±")
            
        except Exception as e:
            logger.error(f"Client baÅŸlatma hatasÄ±: {str(e)}")
            raise
    
    def import_documents_to_datastore(self, data_store_id: str, batch_files: List[Path]) -> bool:
        """Batch dosyalarÄ±nÄ± data store'a import eder"""
        logger.info(f"DokÃ¼manlar data store'a import ediliyor: {data_store_id}")
        
        try:
            # Her batch dosyasÄ±nÄ± iÅŸle
            for batch_file in batch_files:
                logger.info(f"Batch dosyasÄ± iÅŸleniyor: {batch_file}")
                
                # DosyayÄ± Cloud Storage'a yÃ¼kle
                bucket_name = STORAGE_BUCKET_NAME
                blob_name = f"{STORAGE_BATCH_PREFIX}/{batch_file.name}"
                
                bucket = self.storage_client.bucket(bucket_name)
                blob = bucket.blob(blob_name)
                
                if not blob.exists():
                    blob.upload_from_filename(str(batch_file))
                    logger.info(f"Dosya yÃ¼klendi: gs://{bucket_name}/{blob_name}")
                
                # Import iÅŸlemi iÃ§in request oluÅŸtur
                request = discoveryengine.ImportDocumentsRequest(
                    parent=f"projects/{self.project_id}/locations/{self.location}/collections/default_collection/dataStores/{data_store_id}/branches/default_branch",
                    gcs_source=discoveryengine.GcsSource(
                        input_uris=[f"gs://{bucket_name}/{blob_name}"],
                        data_schema="document"
                    ),
                    reconciliation_mode=discoveryengine.ImportDocumentsRequest.ReconciliationMode.INCREMENTAL
                )
                
                # Import iÅŸlemini baÅŸlat
                operation = self.document_client.import_documents(request=request)
                logger.info(f"Import iÅŸlemi baÅŸlatÄ±ldÄ±: {batch_file.name}")
                
                # Ä°ÅŸlemin tamamlanmasÄ±nÄ± bekle (opsiyonel)
                try:
                    response = operation.result(timeout=1800)  # 30 dakika timeout
                    logger.info(f"âœ… Import tamamlandÄ±: {batch_file.name}")
                except Exception as e:
                    logger.warning(f"Import iÅŸlemi timeout oldu, arka planda devam ediyor: {batch_file.name}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ DokÃ¼man import hatasÄ±: {str(e)}")
            return False
    
    def search_documents(self, engine_id: str, query: str, max_results: int = 10) -> List[Dict]:
        """Arama motoru Ã¼zerinde arama yapar"""
        logger.info(f"Arama yapÄ±lÄ±yor: '{query}' (max {max_results} sonuÃ§)")
        
        try:
            # Arama isteÄŸi oluÅŸtur
            request = discoveryengine.SearchRequest(
                serving_config=f"projects/{self.project_id}/locations/{self.location}/collections/default_collection/engines/{engine_id}/servingConfigs/default_config",
                query=query,
                page_size=max_results,
                query_expansion_spec=discoveryengine.SearchRequest.QueryExpansionSpec(
                    condition=discoveryengine.SearchRequest.QueryExpansionSpec.Condition.AUTO
                ),
                spell_correction_spec=discoveryengine.SearchRequest.SpellCorrectionSpec(
                    mode=discoveryengine.SearchRequest.SpellCorrectionSpec.Mode.AUTO
                ),
                content_search_spec=discoveryengine.SearchRequest.ContentSearchSpec(
                    snippet_spec=discoveryengine.SearchRequest.ContentSearchSpec.SnippetSpec(
                        return_snippet=True,
                        max_snippet_count=3
                    ),
                    summary_spec=discoveryengine.SearchRequest.ContentSearchSpec.SummarySpec(
                        summary_result_count=5,
                        include_citations=True
                    )
                )
            )
            
            # Arama yap
            response = self.search_client.search(request=request)
            
            results = []
            for result in response.results:
                doc_data = {
                    'id': result.id,
                    'document': result.document.name if result.document else "",
                    'uri': result.document.derived_struct_data.get('link', '') if result.document and result.document.derived_struct_data else "",
                    'title': result.document.derived_struct_data.get('title', '') if result.document and result.document.derived_struct_data else "",
                    'snippet': result.document.derived_struct_data.get('snippet', '') if result.document and result.document.derived_struct_data else "",
                    'relevance_score': getattr(result, 'relevance_score', 0.0)
                }
                results.append(doc_data)
            
            # Summary bilgisi
            summary_info = {
                'total_results': len(results),
                'query': query,
                'search_time': datetime.now().isoformat(),
                'summary': response.summary.summary_text if hasattr(response, 'summary') and response.summary else ""
            }
            
            logger.info(f"âœ… Arama tamamlandÄ±: {len(results)} sonuÃ§ bulundu")
            
            return {
                'summary': summary_info,
                'results': results
            }
            
        except Exception as e:
            logger.error(f"âŒ Arama hatasÄ±: {str(e)}")
            return {'summary': {}, 'results': []}
    
    def analyze_ai_overview_potential(self, search_results: Dict, target_keywords: List[str]) -> Dict:
        """AI Overview potansiyelini analiz eder"""
        logger.info("AI Overview potansiyeli analiz ediliyor...")
        
        try:
            results = search_results.get('results', [])
            if not results:
                return {'score': 0, 'recommendations': ['Arama sonucu bulunamadÄ±']}
            
            analysis = {
                'timestamp': datetime.now().isoformat(),
                'total_documents': len(results),
                'ai_overview_score': 0,
                'content_quality_score': 0,
                'keyword_relevance_score': 0,
                'recommendations': [],
                'top_performing_content': [],
                'optimization_suggestions': []
            }
            
            # Ä°Ã§erik kalitesi analizi
            content_texts = []
            titles = []
            for result in results:
                content_texts.append(result.get('snippet', '') + ' ' + result.get('title', ''))
                titles.append(result.get('title', ''))
            
            if content_texts:
                # TF-IDF analizi
                vectorizer = TfidfVectorizer(max_features=100, stop_words='english')
                try:
                    tfidf_matrix = vectorizer.fit_transform(content_texts)
                    feature_names = vectorizer.get_feature_names_out()
                    
                    # En Ã¶nemli kelimeleri bul
                    mean_scores = np.mean(tfidf_matrix.toarray(), axis=0)
                    top_terms = [feature_names[i] for i in mean_scores.argsort()[-10:][::-1]]
                    analysis['top_terms'] = top_terms
                    
                except Exception as e:
                    logger.warning(f"TF-IDF analizi baÅŸarÄ±sÄ±z: {str(e)}")
                    analysis['top_terms'] = []
                
                # Sentence similarity analizi
                try:
                    if target_keywords:
                        query_text = ' '.join(target_keywords)
                        query_embedding = self.sentence_model.encode([query_text])
                        content_embeddings = self.sentence_model.encode(content_texts)
                        
                        similarities = cosine_similarity(query_embedding, content_embeddings)[0]
                        analysis['keyword_relevance_score'] = float(np.mean(similarities))
                        
                        # En alakalÄ± iÃ§erikleri bul
                        top_indices = similarities.argsort()[-3:][::-1]
                        analysis['top_performing_content'] = [
                            {
                                'title': titles[i],
                                'similarity_score': float(similarities[i]),
                                'uri': results[i].get('uri', '')
                            }
                            for i in top_indices
                        ]
                    
                except Exception as e:
                    logger.warning(f"Semantic analiz baÅŸarÄ±sÄ±z: {str(e)}")
                    analysis['keyword_relevance_score'] = 0
            
            # Ä°Ã§erik kalitesi skorlarÄ±
            quality_factors = []
            
            for result in results:
                title = result.get('title', '')
                snippet = result.get('snippet', '')
                
                # BaÅŸlÄ±k kalitesi
                title_score = min(len(title.split()), 10) / 10 if title else 0
                
                # Ä°Ã§erik uzunluÄŸu
                content_length_score = min(len(snippet.split()), 50) / 50 if snippet else 0
                
                # Relevans skoru
                relevance_score = result.get('relevance_score', 0)
                
                quality_score = (title_score + content_length_score + relevance_score) / 3
                quality_factors.append(quality_score)
            
            analysis['content_quality_score'] = float(np.mean(quality_factors)) if quality_factors else 0
            
            # Genel AI Overview skoru
            analysis['ai_overview_score'] = (
                analysis['content_quality_score'] * 0.4 +
                analysis['keyword_relevance_score'] * 0.6
            )
            
            # Ã–neriler oluÅŸtur
            if analysis['ai_overview_score'] < 0.3:
                analysis['recommendations'].extend([
                    "Ä°Ã§erik kalitesini artÄ±rÄ±n",
                    "Daha uzun ve detaylÄ± aÃ§Ä±klamalar ekleyin",
                    "Hedef anahtar kelimelerle daha uyumlu iÃ§erik oluÅŸturun"
                ])
            elif analysis['ai_overview_score'] < 0.6:
                analysis['recommendations'].extend([
                    "BaÅŸlÄ±klarÄ± optimize edin",
                    "Meta aÃ§Ä±klamalarÄ± geliÅŸtirin",
                    "YapÄ±landÄ±rÄ±lmÄ±ÅŸ veri ekleyin"
                ])
            else:
                analysis['recommendations'].extend([
                    "Mevcut kaliteyi koruyun",
                    "Ä°Ã§erikleri dÃ¼zenli olarak gÃ¼ncelleyin",
                    "Yeni anahtar kelimeler iÃ§in geniÅŸletme yapÄ±n"
                ])
            
            # Optimizasyon Ã¶nerileri
            analysis['optimization_suggestions'] = [
                "H1-H6 baÅŸlÄ±k yapÄ±sÄ±nÄ± iyileÅŸtirin",
                "FAQ bÃ¶lÃ¼mleri ekleyin",
                "Listeleme formatÄ±nÄ± kullanÄ±n",
                "DoÄŸrudan cevap veren paragraflar yazÄ±n",
                "Ä°statistikler ve veri noktalarÄ± ekleyin"
            ]
            
            logger.info(f"âœ… AI Overview analizi tamamlandÄ±. Skor: {analysis['ai_overview_score']:.2f}")
            
            return analysis
            
        except Exception as e:
            logger.error(f"âŒ AI Overview analiz hatasÄ±: {str(e)}")
            return {'score': 0, 'recommendations': ['Analiz sÄ±rasÄ±nda hata oluÅŸtu']}
    
    def generate_optimization_report(self, analysis: Dict, search_results: Dict) -> str:
        """Optimizasyon raporu oluÅŸturur"""
        report_lines = [
            "# AI OVERVIEW OPTÄ°MÄ°ZASYON RAPORU",
            f"OluÅŸturulma Tarihi: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "=" * 50,
            "",
            "## GENEL DURUM",
            f"AI Overview Skoru: {analysis.get('ai_overview_score', 0):.1%}",
            f"Ä°Ã§erik Kalitesi: {analysis.get('content_quality_score', 0):.1%}",
            f"Anahtar Kelime Uyumu: {analysis.get('keyword_relevance_score', 0):.1%}",
            f"Toplam DokÃ¼man: {analysis.get('total_documents', 0)}",
            "",
            "## EN Ã‡OK KULLANTLAN KELÄ°MELER",
        ]
        
        # En Ã§ok kullanÄ±lan kelimeler
        top_terms = analysis.get('top_terms', [])
        for i, term in enumerate(top_terms[:5], 1):
            report_lines.append(f"{i}. {term}")
        
        report_lines.extend([
            "",
            "## EN Ä°YÄ° PERFORMANS GÃ–STEREN Ä°Ã‡ERÄ°KLER",
        ])
        
        # En iyi iÃ§erikler
        top_content = analysis.get('top_performing_content', [])
        for i, content in enumerate(top_content, 1):
            report_lines.extend([
                f"{i}. {content.get('title', 'BaÅŸlÄ±k yok')}",
                f"   Benzerlik Skoru: {content.get('similarity_score', 0):.1%}",
                f"   URL: {content.get('uri', 'URL yok')}",
                ""
            ])
        
        report_lines.extend([
            "## Ã–NERÄ°LER",
        ])
        
        # Ã–neriler
        recommendations = analysis.get('recommendations', [])
        for i, rec in enumerate(recommendations, 1):
            report_lines.append(f"{i}. {rec}")
        
        report_lines.extend([
            "",
            "## OPTÄ°MÄ°ZASYON Ã–NERÄ°LERÄ°",
        ])
        
        # Optimizasyon Ã¶nerileri
        suggestions = analysis.get('optimization_suggestions', [])
        for i, suggestion in enumerate(suggestions, 1):
            report_lines.append(f"{i}. {suggestion}")
        
        return "\n".join(report_lines)
    
    def save_analysis_results(self, analysis: Dict, search_results: Dict, filename: str = None) -> Path:
        """Analiz sonuÃ§larÄ±nÄ± dosyaya kaydeder"""
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"ai_overview_analysis_{timestamp}.json"
        
        # Analiz sonuÃ§larÄ±
        results = {
            'analysis': analysis,
            'search_results': search_results,
            'generated_at': datetime.now().isoformat(),
            'project_id': self.project_id
        }
        
        # JSON dosyasÄ± kaydet
        filepath = PROCESSED_DATA_DIR / filename
        filepath.parent.mkdir(parents=True, exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # Rapor dosyasÄ± kaydet
        report_filename = filename.replace('.json', '_report.txt')
        report_filepath = PROCESSED_DATA_DIR / report_filename
        
        report_text = self.generate_optimization_report(analysis, search_results)
        with open(report_filepath, 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        logger.info(f"Analiz sonuÃ§larÄ± kaydedildi: {filepath}")
        logger.info(f"Rapor kaydedildi: {report_filepath}")
        
        return filepath

def main():
    """Ana fonksiyon - komut satÄ±rÄ±ndan Ã§alÄ±ÅŸtÄ±rma"""
    import argparse
    
    parser = argparse.ArgumentParser(description='AI Overview arama motoru analizi')
    parser.add_argument('--engine-id', required=True, help='Search engine ID')
    parser.add_argument('--data-store-id', help='Data store ID (import iÃ§in)')
    parser.add_argument('--query', required=True, help='Arama sorgusu')
    parser.add_argument('--keywords', nargs='+', help='Hedef anahtar kelimeler')
    parser.add_argument('--batch-files', nargs='+', help='Import edilecek batch dosyalarÄ±')
    parser.add_argument('--import-only', action='store_true', help='Sadece import iÅŸlemi yap')
    
    args = parser.parse_args()
    
    # Search engine builder baÅŸlat
    builder = SearchEngineBuilder()
    
    try:
        # Batch dosyalarÄ±nÄ± import et (gerekirse)
        if args.batch_files and args.data_store_id:
            batch_paths = [Path(f) for f in args.batch_files]
            logger.info("Batch dosyalarÄ± import ediliyor...")
            success = builder.import_documents_to_datastore(args.data_store_id, batch_paths)
            
            if not success:
                print("âŒ Import iÅŸlemi baÅŸarÄ±sÄ±z!")
                return
            
            if args.import_only:
                print("âœ… Import iÅŸlemi tamamlandÄ±!")
                return
        
        # Arama yap
        print(f"\nğŸ” Arama yapÄ±lÄ±yor: '{args.query}'")
        search_results = builder.search_documents(args.engine_id, args.query)
        
        if not search_results.get('results'):
            print("âŒ Arama sonucu bulunamadÄ±!")
            return
        
        # AI Overview analizi
        print(f"\nğŸ¤– AI Overview analizi yapÄ±lÄ±yor...")
        keywords = args.keywords or [args.query]
        analysis = builder.analyze_ai_overview_potential(search_results, keywords)
        
        # SonuÃ§larÄ± kaydet
        result_file = builder.save_analysis_results(analysis, search_results)
        
        # Ã–zet rapor
        print(f"\nğŸ“Š ANALIZ SONUÃ‡LARI")
        print(f"{'='*40}")
        print(f"AI Overview Skoru: {analysis.get('ai_overview_score', 0):.1%}")
        print(f"Ä°Ã§erik Kalitesi: {analysis.get('content_quality_score', 0):.1%}")
        print(f"Anahtar Kelime Uyumu: {analysis.get('keyword_relevance_score', 0):.1%}")
        print(f"Bulunan DokÃ¼man SayÄ±sÄ±: {len(search_results.get('results', []))}")
        
        print(f"\nğŸ’¡ EN Ã–NEMLÄ° Ã–NERÄ°LER:")
        recommendations = analysis.get('recommendations', [])
        for i, rec in enumerate(recommendations[:3], 1):
            print(f"  {i}. {rec}")
        
        print(f"\nğŸ“ DetaylÄ± rapor: {result_file}")
        print(f"ğŸ“„ Metin raporu: {result_file.with_suffix('')}_report.txt")
        
    except Exception as e:
        logger.error(f"Ä°ÅŸlem hatasÄ±: {str(e)}")
        print(f"âŒ Hata: {str(e)}")

if __name__ == "__main__":
    main() 